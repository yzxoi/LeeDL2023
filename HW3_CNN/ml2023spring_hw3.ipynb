{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "%pip install kagglehub\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml2023spring_hw3_path = kagglehub.competition_download('ml2023spring-hw3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "IPython.notebook.set_autosave_interval(5000)"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autosaving every 5 seconds\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import logging\n",
        "\n",
        "nblog = open(\"nb.log\", \"a+\")\n",
        "sys.stdout.echo = nblog\n",
        "sys.stderr.echo = nblog\n",
        "\n",
        "get_ipython().log.handlers[0].stream = nblog\n",
        "get_ipython().log.setLevel(logging.INFO)\n",
        "\n",
        "%autosave 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hhh39Dp59Dx"
      },
      "source": [
        "## HW3 Image Classification\n",
        "#### Solve image classification with convolutional neural networks(CNN).\n",
        "#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:41:55.293458Z",
          "iopub.status.busy": "2023-03-06T12:41:55.292615Z",
          "iopub.status.idle": "2023-03-06T12:41:56.414165Z",
          "shell.execute_reply": "2023-03-06T12:41:56.412686Z",
          "shell.execute_reply.started": "2023-03-06T12:41:55.293416Z"
        },
        "id": "0gyltofQ59Dz",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 19 23:00:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 575.51.03              Driver Version: 576.02         CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:07:00.0 Off |                  N/A |\n",
            "| N/A   32C    P8              3W /   68W |     392MiB /   8188MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check GPU type.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10SGyA_D59Dz"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:01.638935Z",
          "iopub.status.busy": "2023-03-06T12:42:01.637686Z",
          "iopub.status.idle": "2023-03-06T12:42:01.644205Z",
          "shell.execute_reply": "2023-03-06T12:42:01.642821Z",
          "shell.execute_reply.started": "2023-03-06T12:42:01.638884Z"
        },
        "id": "AJnJI2RZ59Dz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "_exp_name = \"hw3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:03.99447Z",
          "iopub.status.busy": "2023-03-06T12:42:03.993501Z",
          "iopub.status.idle": "2023-03-06T12:42:06.602986Z",
          "shell.execute_reply": "2023-03-06T12:42:06.601849Z",
          "shell.execute_reply.started": "2023-03-06T12:42:03.994417Z"
        },
        "id": "lXsN0nNp59Dz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:08.671999Z",
          "iopub.status.busy": "2023-03-06T12:42:08.671281Z",
          "iopub.status.idle": "2023-03-06T12:42:08.735685Z",
          "shell.execute_reply": "2023-03-06T12:42:08.734697Z",
          "shell.execute_reply.started": "2023-03-06T12:42:08.671963Z"
        },
        "id": "d6JXTCBr59D0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "myseed = 1314520  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ywp1PBR59D0"
      },
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:10.865746Z",
          "iopub.status.busy": "2023-03-06T12:42:10.864761Z",
          "iopub.status.idle": "2023-03-06T12:42:10.871901Z",
          "shell.execute_reply": "2023-03-06T12:42:10.870886Z",
          "shell.execute_reply.started": "2023-03-06T12:42:10.865706Z"
        },
        "id": "KER83e4I59D0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Normally, We don't need augmentations in testing and validation.\n",
        "# All we need here is to resize the PIL image and transform it into Tensor.\n",
        "test_tfm = transforms.Compose([\n",
        "\ttransforms.Resize(256),\n",
        "    transforms.CenterCrop(224),  # Resize the image into a fixed shape (height = width = 224)\n",
        "    transforms.ToTensor(),\n",
        "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406],       # Normalize to ImageNet stats\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# However, it is also possible to use augmentation in the testing phase.\n",
        "# You may use train_tfm to produce a variety of images and then test using ensemble methods\n",
        "train_tfm = transforms.Compose([\n",
        "    # Resize the image into a fixed shape (height = width = 128)\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),   # Random crop & resize\n",
        "    transforms.RandomHorizontalFlip(p=0.5),                # Flip horizontally\n",
        "    transforms.RandomRotation(15),                         # Random rotation ±15°\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3)\n",
        "    ], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.1),                     # 10% chance to grayscale\n",
        "    transforms.ToTensor(),                                 # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],       # Normalize to ImageNet stats\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aVVNbU759D0"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:13.259609Z",
          "iopub.status.busy": "2023-03-06T12:42:13.258557Z",
          "iopub.status.idle": "2023-03-06T12:42:13.26865Z",
          "shell.execute_reply": "2023-03-06T12:42:13.267304Z",
          "shell.execute_reply.started": "2023-03-06T12:42:13.259548Z"
        },
        "id": "vOmcaikh59D0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class FoodDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path=None, tfm=test_tfm, files=None, labels=None):\n",
        "        super(FoodDataset).__init__()\n",
        "        self.transform = tfm\n",
        "\n",
        "        if files is not None:\n",
        "            self.files = files\n",
        "            self.labels = labels  # 训练用\n",
        "        else:\n",
        "            # 自动加载路径\n",
        "            self.files = sorted([\n",
        "                os.path.join(path, x) for x in os.listdir(path)\n",
        "                if x.endswith(\".jpg\")\n",
        "            ])\n",
        "            try:\n",
        "                # 推测标签（用于 train/valid）\n",
        "                self.labels = [int(f.split(\"/\")[-1].split(\"_\")[0]) for f in self.files]\n",
        "            except:\n",
        "                self.labels = [-1] * len(self.files)  # 测试集无标签\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        im = Image.open(fname)\n",
        "        if self.transform:\n",
        "            im = self.transform(im)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return im, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxES3LbC59D0"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:16.971206Z",
          "iopub.status.busy": "2023-03-06T12:42:16.970826Z",
          "iopub.status.idle": "2023-03-06T12:42:16.985456Z",
          "shell.execute_reply": "2023-03-06T12:42:16.981608Z",
          "shell.execute_reply.started": "2023-03-06T12:42:16.971173Z"
        },
        "id": "qxa-ymUw59D0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
        "        # input 維度 [3, 128, 128]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            # nn.Linear(512*4*4, 1024),\n",
        "            nn.Linear(512*7*7, 1024),    # input 224x224\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 11)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n",
        "    \n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "class MobileNetV2Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super().__init__()\n",
        "        self.model = mobilenet_v2(weights=None)  # or weights='DEFAULT' if允许预训练\n",
        "        self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from torchvision.models import efficientnet_b0\n",
        "\n",
        "class EfficientNetB0Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super().__init__()\n",
        "        self.model = efficientnet_b0(weights=None)\n",
        "        self.model.classifier[1] = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class ResNet18Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=11):\n",
        "        super().__init__()\n",
        "        self.model = resnet18(weights=None)\n",
        "        self.model.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RlL9LmY59D1"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:48.157271Z",
          "iopub.status.busy": "2023-03-06T12:42:48.156224Z",
          "iopub.status.idle": "2023-03-06T12:42:48.301646Z",
          "shell.execute_reply": "2023-03-06T12:42:48.300565Z",
          "shell.execute_reply.started": "2023-03-06T12:42:48.157219Z"
        },
        "id": "WFZkqrx159D1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# \"cuda\" only when GPUs are available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize a model, and put it on the device specified.\n",
        "# model = Classifier().to(device)\n",
        "# Using Kfold, we will new a model for each fold.\n",
        "\n",
        "# Using custom model\n",
        "# from torchvision.models import resnet50, resnet18\n",
        "# model = resnet18(weights=None).to(device)\n",
        "\n",
        "# The number of batch size.\n",
        "batch_size = 16\n",
        "\n",
        "# The number of training epochs.\n",
        "n_epochs = 200\n",
        "\n",
        "# If no improvement in 'patience' epochs, early stop.\n",
        "patience = 10\n",
        "\n",
        "# # For the classification task, we use cross-entropy as the measurement of performance.\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTxxtW4559D1"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:50.55635Z",
          "iopub.status.busy": "2023-03-06T12:42:50.555896Z",
          "iopub.status.idle": "2023-03-06T12:42:52.184902Z",
          "shell.execute_reply": "2023-03-06T12:42:52.183686Z",
          "shell.execute_reply.started": "2023-03-06T12:42:50.556303Z"
        },
        "id": "db1q-Ojd59D1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Construct train and valid datasets.\n",
        "# The argument \"loader\" tells how torchvision reads the data.\n",
        "# train_set = FoodDataset(\"./mlhw3/train\", tfm=train_tfm)\n",
        "# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "# valid_set = FoodDataset(\"./mlhw3/valid\", tfm=test_tfm)\n",
        "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "# print(f\"Train set size: {len(train_set)}, Valid set size: {len(valid_set)}\")\n",
        "\n",
        "# Going to use KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbxgtiRK59D1"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:42:58.525574Z",
          "iopub.status.busy": "2023-03-06T12:42:58.525093Z",
          "iopub.status.idle": "2023-03-06T12:47:26.36056Z",
          "shell.execute_reply": "2023-03-06T12:47:26.358987Z",
          "shell.execute_reply.started": "2023-03-06T12:42:58.525522Z"
        },
        "id": "dEkc0-DP59D1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def trainer(fold, train_loader, valid_loader, model, device, writer, val_accs, model_name):\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n",
        "    stale = 0\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # ---------- Training ----------\n",
        "        # Make sure the model is in train mode before training.\n",
        "        model.train()\n",
        "\n",
        "        # These are used to record information in training.\n",
        "        train_loss = []\n",
        "        train_accs = []\n",
        "\n",
        "        train_loader_bar = tqdm(train_loader, desc=f\"Fold {fold}, Model {model_name} - Training Epoch {epoch + 1}/{n_epochs}\")\n",
        "\n",
        "        for batch in train_loader_bar:\n",
        "\n",
        "            # A batch consists of image data and corresponding labels.\n",
        "            imgs, labels = batch\n",
        "            #imgs = imgs.half()\n",
        "            #print(imgs.shape,labels.shape)\n",
        "\n",
        "            # Forward the data. (Make sure data and model are on the same device.)\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "            # Calculate the cross-entropy loss.\n",
        "            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
        "            loss = criterion(logits, labels.to(device))\n",
        "\n",
        "            # Gradients stored in the parameters in the previous step should be cleared out first.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute the gradients for parameters.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the gradient norms for stable training.\n",
        "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "            # Update the parameters with computed gradients.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute the accuracy for current batch.\n",
        "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "            # Record the loss and accuracy.\n",
        "            train_loss.append(loss.item())\n",
        "            train_accs.append(acc)\n",
        "            train_loader_bar.set_postfix(\n",
        "                loss=f\"{sum(train_loss)/len(train_loss):.5f}\",\n",
        "                acc=f\"{sum(train_accs)/len(train_accs):.5f}\"\n",
        "            )\n",
        "\n",
        "        train_loss = sum(train_loss) / len(train_loss)\n",
        "        train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "        # Print the information.\n",
        "        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "        # ---------- Validation ----------\n",
        "        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
        "        model.eval()\n",
        "\n",
        "        # These are used to record information in validation.\n",
        "        valid_loss = []\n",
        "        valid_accs = []\n",
        "        valid_loader_bar = tqdm(valid_loader, desc=f\"Fold {fold}, Model {model_name} - Validation Epoch {epoch + 1}/{n_epochs}\")\n",
        "\n",
        "        # Iterate the validation set by batches.\n",
        "        for batch in valid_loader_bar:\n",
        "\n",
        "            # A batch consists of image data and corresponding labels.\n",
        "            imgs, labels = batch\n",
        "            #imgs = imgs.half()\n",
        "\n",
        "            # We don't need gradient in validation.\n",
        "            # Using torch.no_grad() accelerates the forward process.\n",
        "            with torch.no_grad():\n",
        "                logits = model(imgs.to(device))\n",
        "\n",
        "            # We can still compute the loss (but not the gradient).\n",
        "            loss = criterion(logits, labels.to(device))\n",
        "\n",
        "            # Compute the accuracy for current batch.\n",
        "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "            # Record the loss and accuracy.\n",
        "            valid_loss.append(loss.item())\n",
        "            valid_accs.append(acc)\n",
        "            valid_loader_bar.set_postfix(\n",
        "                loss=f\"{sum(valid_loss)/len(valid_loss):.5f}\",\n",
        "                acc=f\"{sum(valid_accs)/len(valid_accs):.5f}\"\n",
        "            )\n",
        "            #break\n",
        "\n",
        "        # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "        # Print the information.\n",
        "        print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "        # — Log to TensorBoard under fold subdir —\n",
        "        writer.add_scalar(f\"Fold{fold}/Loss/Train\", train_loss, epoch)\n",
        "        writer.add_scalar(f\"Fold{fold}/Loss/Val\",   valid_loss, epoch)\n",
        "        writer.add_scalar(f\"Fold{fold}/Acc/Train\",  train_acc, epoch)\n",
        "        writer.add_scalar(f\"Fold{fold}/Acc/Val\",    valid_acc, epoch)\n",
        "        writer.add_scalar(f\"Fold{fold}/LR\", optimizer.param_groups[0]['lr'], epoch)\n",
        "        scheduler.step(valid_loss)  # Step the scheduler with validation loss\n",
        "\n",
        "        # update logs\n",
        "        if valid_acc > best_acc:\n",
        "            with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "        else:\n",
        "            with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "        # save models\n",
        "        if valid_acc > best_acc:\n",
        "            # if len(val_accs) == 0 or valid_acc > max(val_accs):\n",
        "            #     torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\")  # save the best model\n",
        "            print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "            torch.save(model.state_dict(), f\"{_exp_name}_fold{fold}_model{model_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "            best_acc = valid_acc\n",
        "            stale = 0\n",
        "        else:\n",
        "            stale += 1\n",
        "            if stale > patience:\n",
        "                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "                break\n",
        "    return best_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] Train set size: 13643\n",
            "=== Fold 1/5 ===\n",
            "[info] Train set size: 10914, Valid set size: 2729\n",
            "\n",
            "--- Training Model 0 (Classifier) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80c1852b82634554a3294b1b23a5fa1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 1/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 001/200 ] loss = 2.30660, acc = 0.20342\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04232429b3df47f28b8d1135078d6db1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 1/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 001/200 ] loss = 2.13329, acc = 0.28107\n",
            "[ Valid | 001/200 ] loss = 2.13329, acc = 0.28107 -> best\n",
            "Best model found at epoch 0, saving model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b121522a5aeb4892b8db8fd77df2496a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 2/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 002/200 ] loss = 2.17369, acc = 0.25430\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd03bc3214684f30b7348193b19b1ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 2/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>AssertionError\n",
            "\n",
            ": Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "can only test a child process  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "        self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        if w.is_alive():if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError\n",
            "AssertionError: : can only test a child processcan only test a child process\n",
            "\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 002/200 ] loss = 2.12199, acc = 0.28273\n",
            "[ Valid | 002/200 ] loss = 2.12199, acc = 0.28273 -> best\n",
            "Best model found at epoch 1, saving model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4537813dcf9a43cc9a59208a69334b53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 3/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "                self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "                if w.is_alive():if w.is_alive():if w.is_alive():if w.is_alive():\n",
            "\n",
            "\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "                assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "\n",
            "\n",
            "AssertionErrorAssertionErrorAssertionError: AssertionError: can only test a child process: can only test a child process: \n",
            "can only test a child processcan only test a child process\n",
            "\n",
            "\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    \n",
            "Traceback (most recent call last):\n",
            "        self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "self._shutdown_workers()self._shutdown_workers()    \n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "            \n",
            "if w.is_alive():if w.is_alive():if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():            \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Exception ignored in: \n",
            "\n",
            "AssertionError    <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: AssertionError\n",
            "\n",
            ": : can only test a child processTraceback (most recent call last):\n",
            "AssertionErrorcan only test a child processcan only test a child process\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            ": \n",
            "\n",
            "can only test a child process    \n",
            "self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: Exception ignored in: can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 003/200 ] loss = 2.12115, acc = 0.27983\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22720e08bdeb4de18415602990824f5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 3/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 003/200 ] loss = 2.00838, acc = 0.33882\n",
            "[ Valid | 003/200 ] loss = 2.00838, acc = 0.33882 -> best\n",
            "Best model found at epoch 2, saving model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9054aac8222e48a4afba793479ca9fcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 4/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 004/200 ] loss = 2.07597, acc = 0.29493\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7de8e7be55f34c83adf045ef7f8d4f8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 4/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 004/200 ] loss = 2.00779, acc = 0.32566\n",
            "[ Valid | 004/200 ] loss = 2.00779, acc = 0.32566\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "032c1ac235524818b313b6cbccbab11a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 5/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 005/200 ] loss = 2.03771, acc = 0.32147\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6eb3a9ccac064e7b8bc3caade68bd45a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 5/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "        self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()    \n",
            "\n",
            "\n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "          File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():if w.is_alive():if w.is_alive():    \n",
            "\n",
            "\n",
            "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "AssertionError\n",
            "AssertionErrorAssertionError: AssertionError: can only test a child process: can only test a child process: can only test a child process\n",
            "can only test a child process\n",
            "\n",
            "\n",
            "Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Exception ignored in: \n",
            "\n",
            "\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Traceback (most recent call last):\n",
            "              File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()    \n",
            "\n",
            "\n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "              File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "if w.is_alive():if w.is_alive():if w.is_alive():\n",
            "    \n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "            assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
            "\n",
            "\n",
            "AssertionErrorAssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: : \n",
            ": can only test a child processcan only test a child processAssertionErrorcan only test a child process\n",
            ": \n",
            "\n",
            "can only test a child process\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Exception ignored in: \n",
            "Traceback (most recent call last):\n",
            "\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "Traceback (most recent call last):\n",
            "        self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "self._shutdown_workers()self._shutdown_workers()\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "self._shutdown_workers()      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "if w.is_alive():          File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "if w.is_alive():if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    \n",
            "\n",
            "    if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "        \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError    \n",
            "\n",
            ": assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionErrorAssertionErrorcan only test a child process\n",
            ": : \n",
            "can only test a child processAssertionErrorcan only test a child process\n",
            ": \n",
            "can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 005/200 ] loss = 1.92024, acc = 0.40006\n",
            "[ Valid | 005/200 ] loss = 1.92024, acc = 0.40006 -> best\n",
            "Best model found at epoch 4, saving model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: "
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02bdb316bcc645478fbe9ffd35bef835",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 6/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "                self._shutdown_workers()self._shutdown_workers()self._shutdown_workers()\n",
            "self._shutdown_workers()\n",
            "\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "                if w.is_alive():if w.is_alive():if w.is_alive():if w.is_alive():\n",
            "\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "                assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "\n",
            "\n",
            "AssertionErrorAssertionErrorAssertionErrorAssertionError: : : : can only test a child processcan only test a child processcan only test a child processcan only test a child process\n",
            "\n",
            "\n",
            "\n",
            "Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "                self._shutdown_workers()self._shutdown_workers()\n",
            "self._shutdown_workers()self._shutdown_workers()\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "        if w.is_alive():    if w.is_alive():if w.is_alive():\n",
            "if w.is_alive():\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: AssertionError\n",
            "can only test a child process: AssertionError: can only test a child process\n",
            ": \n",
            "can only test a child processcan only test a child process\n",
            "\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30><function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Exception ignored in: \n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "\n",
            "        Traceback (most recent call last):\n",
            "self._shutdown_workers()Traceback (most recent call last):\n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    \n",
            "self._shutdown_workers()      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "self._shutdown_workers()          File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "if w.is_alive():if w.is_alive():  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    \n",
            "\n",
            "if w.is_alive():      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "if w.is_alive():          File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "    \n",
            "AssertionErrorAssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: : \n",
            ": can only test a child processcan only test a child processAssertionErrorcan only test a child process\n",
            "\n",
            ": \n",
            "can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 006/200 ] loss = 1.99544, acc = 0.34224\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18905ed821c24c77af2d154928426ec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 6/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 006/200 ] loss = 1.94375, acc = 0.39786\n",
            "[ Valid | 006/200 ] loss = 1.94375, acc = 0.39786\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49dad37788f6408192f1e78cdcfe9b83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 7/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Train | 007/200 ] loss = 1.96534, acc = 0.35542\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1611beeb5f9498fac2e2f0abc009fa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Validation Epoch 7/200:   0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Valid | 007/200 ] loss = 1.89528, acc = 0.40643\n",
            "[ Valid | 007/200 ] loss = 1.89528, acc = 0.40643 -> best\n",
            "Best model found at epoch 6, saving model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0b1b0ff9cd04c5596f4065a949d0e4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fold 1, Model Classifier - Training Epoch 8/200:   0%|          | 0/683 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    Exception ignored in: if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Traceback (most recent call last):\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    self._shutdown_workers()AssertionError\n",
            ":   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "can only test a child process\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6b68b3cd30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m              \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m              \u001b[49m\u001b[43mval_accs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_accs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m val_accs[model_id]\u001b[38;5;241m.\u001b[39mappend(acc)\n",
            "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(fold, train_loader, valid_loader, model, device, writer, val_accs, model_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m train_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m train_loader_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader_bar:\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# A batch consists of image data and corresponding labels.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     imgs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#imgs = imgs.half()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#print(imgs.shape,labels.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Forward the data. (Make sure data and model are on the same device.)\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1191\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1191\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[1;32m   1193\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:262\u001b[0m, in \u001b[0;36mtqdm_notebook.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# cannot catch KeyboardInterrupt when using manual tqdm\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# as the interrupt will most likely happen on another statement\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:157\u001b[0m, in \u001b[0;36mtqdm_notebook.display\u001b[0;34m(self, msg, pos, close, bar_style, check_delay)\u001b[0m\n\u001b[1;32m    154\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_meter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md)\n\u001b[1;32m    156\u001b[0m ltext, pbar, rtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mchildren\n\u001b[0;32m--> 157\u001b[0m \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msg:\n\u001b[1;32m    160\u001b[0m     msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\u2007\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# fix html space padding\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py:716\u001b[0m, in \u001b[0;36mTraitType.__set__\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_only:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraitError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m trait is read-only.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py:706\u001b[0m, in \u001b[0;36mTraitType.set\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m     silent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# we explicitly compare silent to True just in case the equality\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# comparison above returns something other than True/False\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m     \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_notify_trait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/traitlets/traitlets.py:1513\u001b[0m, in \u001b[0;36mHasTraits._notify_trait\u001b[0;34m(self, name, old_value, new_value)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_notify_trait\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, old_value: t\u001b[38;5;241m.\u001b[39mAny, new_value: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotify_change\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget.py:700\u001b[0m, in \u001b[0;36mWidget.notify_change\u001b[0;34m(self, change)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# Make sure this isn't information that the front-end just sent us.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_send_property(name, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)):\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# Send new state to front-end\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mnotify_change(change)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget.py:586\u001b[0m, in \u001b[0;36mWidget.send_state\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    584\u001b[0m state, buffer_paths, buffers \u001b[38;5;241m=\u001b[39m _remove_buffers(state)\n\u001b[1;32m    585\u001b[0m msg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer_paths\u001b[39m\u001b[38;5;124m'\u001b[39m: buffer_paths}\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipywidgets/widgets/widget.py:825\u001b[0m, in \u001b[0;36mWidget._send\u001b[0;34m(self, msg, buffers)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a message to the model in the front-end.\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/comm/base_comm.py:147\u001b[0m, in \u001b[0;36mBaseComm.send\u001b[0;34m(self, data, metadata, buffers)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: MaybeDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, metadata: MaybeDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, buffers: BuffersType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a message to the frontend-side version of this comm\"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_msg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomm_msg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/comm/comm.py:37\u001b[0m, in \u001b[0;36mBaseComm.publish_msg\u001b[0;34m(self, msg_type, data, metadata, buffers, **keys)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m Kernel\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miopub_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mident\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jupyter_client/session.py:852\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_version:\n\u001b[1;32m    851\u001b[0m     msg \u001b[38;5;241m=\u001b[39m adapt(msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_version)\n\u001b[0;32m--> 852\u001b[0m to_send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mident\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m to_send\u001b[38;5;241m.\u001b[39mextend(buffers)\n\u001b[1;32m    854\u001b[0m longest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m to_send])\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jupyter_client/session.py:721\u001b[0m, in \u001b[0;36mSession.serialize\u001b[0;34m(self, msg, ident)\u001b[0m\n\u001b[1;32m    719\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnone\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 721\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# content is already packed, as in a relayed message\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/jupyter_client/session.py:95\u001b[0m, in \u001b[0;36mjson_packer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a json object to a bytes.\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_default\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Fallback to trying to clean the json before serializing\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     packed \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[1;32m    104\u001b[0m         json_clean(obj),\n\u001b[1;32m    105\u001b[0m         default\u001b[38;5;241m=\u001b[39mjson_default,\n\u001b[1;32m    106\u001b[0m         ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m     )\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
            "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:248\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (_one_shot \u001b[38;5;129;01mand\u001b[39;00m c_make_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 248\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m \u001b[43mc_make_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmarkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "models = [\n",
        "    Classifier,\n",
        "    ResNet18Classifier,\n",
        "    MobileNetV2Classifier,\n",
        "    EfficientNetB0Classifier\n",
        "]\n",
        "    \n",
        "all_data_paths = [\"./mlhw3/train\", \"./mlhw3/valid\"]\n",
        "all_data_files = []\n",
        "all_data_labels = []\n",
        "for path in all_data_paths:\n",
        "    dataset = FoodDataset(path, tfm=None)  # 不设 transform\n",
        "    all_data_files.extend(dataset.files)\n",
        "    all_data_labels.extend(dataset.labels)\n",
        "\n",
        "print(f\"[info] Train set size: {len(all_data_files)}\")\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=myseed)\n",
        "val_accs = [[] for _ in range(len(models))]\n",
        "\n",
        "writers = [SummaryWriter(f\"./runs/{_exp_name}_model{model.__name__}\") for model in models]\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(all_data_files)):\n",
        "    print(f\"=== Fold {fold+1}/{n_splits} ===\")\n",
        "\n",
        "    # 构建两个 Dataset，并传入对应 transform\n",
        "    train_dataset = FoodDataset(files=[all_data_files[i] for i in train_idx],\n",
        "                            labels=[all_data_labels[i] for i in train_idx],\n",
        "                            tfm=train_tfm)\n",
        "\n",
        "    valid_dataset = FoodDataset(files=[all_data_files[i] for i in valid_idx],\n",
        "                            labels=[all_data_labels[i] for i in valid_idx],\n",
        "                            tfm=test_tfm)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              shuffle=True, pin_memory=True, num_workers=4)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                              shuffle=False, pin_memory=True, num_workers=4)\n",
        "    \n",
        "    print(f\"[info] Train set size: {len(train_dataset)}, Valid set size: {len(valid_dataset)}\")\n",
        "    \n",
        "    # 对每个模型分别训练\n",
        "    for model_id, model_class in enumerate(models):\n",
        "        print(f\"\\n--- Training Model {model_id} ({model_class.__name__}) ---\")\n",
        "        model = model_class().to(device)\n",
        "        \n",
        "        # 开始训练\n",
        "        acc = trainer(fold=fold+1,\n",
        "                      train_loader=train_loader,\n",
        "                      valid_loader=valid_loader,\n",
        "                      model=model,\n",
        "                      device=device,\n",
        "                      writer=writers[model_id],\n",
        "                      val_accs=val_accs[model_id],\n",
        "                      model_name=model_class.__name__)\n",
        "        \n",
        "        val_accs[model_id].append(acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ8VB30p59D1"
      },
      "source": [
        "### Dataloader for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:47:31.638946Z",
          "iopub.status.busy": "2023-03-06T12:47:31.637882Z",
          "iopub.status.idle": "2023-03-06T12:47:32.085514Z",
          "shell.execute_reply": "2023-03-06T12:47:32.084453Z",
          "shell.execute_reply.started": "2023-03-06T12:47:31.638899Z"
        },
        "id": "cPS8xqtO59D1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set size: 3000, TTA set size: 12000\n"
          ]
        }
      ],
      "source": [
        "# Construct test datasets.\n",
        "# The argument \"loader\" tells how torchvision reads the data.\n",
        "test_set_std = FoodDataset(\"./mlhw3/test\", tfm=test_tfm)\n",
        "test_loader_std = DataLoader(test_set_std, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "tta_repeats = 4\n",
        "test_set_tta = ConcatDataset([\n",
        "    FoodDataset(\"./mlhw3/test\", tfm=train_tfm) for _ in range(tta_repeats)\n",
        "])\n",
        "test_loader_tta = DataLoader(test_set_tta, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "print(f\"Test set size: {len(test_set_std)}, TTA set size: {len(test_set_tta)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7iauntf59D2"
      },
      "source": [
        "### Testing and generate prediction CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:48:40.395319Z",
          "iopub.status.busy": "2023-03-06T12:48:40.394641Z",
          "iopub.status.idle": "2023-03-06T12:49:24.735001Z",
          "shell.execute_reply": "2023-03-06T12:49:24.733969Z",
          "shell.execute_reply.started": "2023-03-06T12:48:40.395279Z"
        },
        "id": "gj-qj4F059D2",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f0ebdff55b641728f06a44f57ed1e11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/188 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m logits_std \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader_std):\n\u001b[1;32m     15\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     16\u001b[0m         logits_std\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39mcpu())\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mFoodDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(fname)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 29\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im, label\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    465\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:2345\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(size, resample, box)\n\u001b[1;32m   2343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m-> 2345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample \u001b[38;5;241m!=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST:\n\u001b[1;32m   2348\u001b[0m     factor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_paths = [\n",
        "    (model_class, f\"{_exp_name}_fold1_model{model_class.__name__}_best.ckpt\")\n",
        "    for model_class in models\n",
        "]\n",
        "\n",
        "# Soft voting accumulation\n",
        "logits_all_std, logits_all_tta = [], []\n",
        "\n",
        "for model_class, path in model_paths:\n",
        "    print(path)\n",
        "    model = model_class().to(device)\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "\n",
        "    # test_tfm 推理\n",
        "    logits_std = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in tqdm(test_loader_std):\n",
        "            logits = model(data.to(device))\n",
        "            logits_std.append(logits.cpu())\n",
        "    logits_all_std.append(torch.cat(logits_std))  # [N, C]\n",
        "\n",
        "    # train_tfm (TTA) 推理\n",
        "    logits_tta = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in tqdm(test_loader_tta):\n",
        "            logits = model(data.to(device))\n",
        "            logits_tta.append(logits.cpu())\n",
        "    logits_cat = torch.cat(logits_tta)  # [N * tta_repeats, C]\n",
        "\n",
        "    # 每 tta_repeats 取平均\n",
        "    N = len(test_set_std)\n",
        "    logits_avg = logits_cat.view(tta_repeats, N, -1).mean(dim=0)  # [N, C]\n",
        "    logits_all_tta.append(logits_avg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T12:49:30.701796Z",
          "iopub.status.busy": "2023-03-06T12:49:30.70078Z",
          "iopub.status.idle": "2023-03-06T12:49:30.718186Z",
          "shell.execute_reply": "2023-03-06T12:49:30.716859Z",
          "shell.execute_reply.started": "2023-03-06T12:49:30.701756Z"
        },
        "id": "JCUKPWEc59D2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#create test csv\n",
        "logits_std_mean = torch.stack(logits_all_std).mean(dim=0)\n",
        "logits_tta_mean = torch.stack(logits_all_tta).mean(dim=0)\n",
        "\n",
        "# 加权融合\n",
        "final_logits = logits_std_mean * 0.8 + logits_tta_mean * 0.2\n",
        "final_preds = final_logits.argmax(dim=1).numpy()\n",
        "\n",
        "# 写入 CSV\n",
        "df = pd.DataFrame()\n",
        "df[\"Id\"] = [f\"{i:04d}\" for i in range(len(test_set_std))]\n",
        "df[\"Category\"] = final_preds\n",
        "df.to_csv(\"submission_tta_loader.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ml2023spring-hw3-sample-code",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
